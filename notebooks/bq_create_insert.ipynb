{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388d49d6-fd0d-447d-a96f-0dba8e4ab5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import LoadJobConfig\n",
    "from pandas_gbq import to_gbq\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google_auth_oauthlib import flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f280a-b926-49b7-b483-493da857af40",
   "metadata": {},
   "source": [
    "#### funciones:\n",
    "\n",
    "- **credentials = service_account.Credentials.from_service_account_file(bq_credentials, scopes=scopes)**\n",
    "- **client = bigquery.Client(credentials=credentials, project=credentials.project_id)**   --> para establecer la conexion\n",
    "---\n",
    "- **client.dataset(dataset_id)**  --> para referenciar el data set\n",
    "- **dataset = client.create_dataset('us_states_dataset')** --> sino existe el dataset lo podemos crear\n",
    "- **client.table(table_id)**   --> para referenciar la tabla\n",
    "- **table = client.create_table(table)** --> sino existe la tabla la podemos crear\n",
    "- o referenciarla de golpe:\n",
    "- **table = client.dataset(dataset_id).table(table_id)** --> para referenciar el dataset y la tabla en conjunto\n",
    "- en el caso de que si que exista:\n",
    "- **table = client.get_table(table)** --> obtenemos la tabla de BigQuery\n",
    "---\n",
    "- **job_config = bigquery.job.QueryJobConfig(use_query_cache=False)**   --> para modificar configuraciones\n",
    "- **job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")**  --> para modificar configuraciones\n",
    "- **job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON** --> para seguir añadiendo modificaciones configuraciones\n",
    "---\n",
    "- **client.query(query, job_config=job_config)** --> para ejecutar una query\n",
    "---\n",
    "- **client.load_table_from_uri(gcs_uri, table, job_config=job_config)** --> para cargar una tabla a BQ desde un bucket de GCS\n",
    "---\n",
    "- CREACION DE TABLA\n",
    "- - **table = client.dataset(dataset_id).table(table_id)** --> referenciamos la tabla que deberia existir en BigQuery\n",
    "  - 0º COMPROBAMOS  a ver si existe **client.get_table(table)** --> obtenemos la tabla de BigQuery. SINO EXISTE LA TENDRIAMOS QUE CREAR:\n",
    "  - 1º DEFINIMOS el schema --> **schema = [bigquery.SchemaField(\"cars\", \"STRING\", mode=\"NULLABLE\"), bigquery.SchemaField(\"mpg\", \"FLOAT\", mode=\"REQUIRED\"),  ...]**\n",
    "  - 2º DEFINIMOS la tabla --> **table = bigquery.Table(table, schema=schema)**\n",
    "  - 3º CREAMOS la tabla --> **table = client.create_table(table)**\n",
    "  - también se podría saltar el paso 1 y 2 y configurar el schema en el jobconfig\n",
    "  - - **job_config = job_config.schema = [bigquery.SchemaField('name', 'STRING'), bigquery.SchemaField('post_abbr','STRING')]**\n",
    "    - **table = client.create_table(table, job_config=job_config)**\n",
    "---\n",
    "- INSERCION DE VALORES (debajo de los ya existentes) (partimos de que ya existe)\n",
    "- - 0º OBTENEMOS la tabla **table = client.get_table(table)**\n",
    "  - 1º CARGAMOS DATOS (desde un DF en este caso) **job = client.load_table_from_dataframe(DF, table)**\n",
    "--- \n",
    "- SOBREESCRIBIR VALORES\n",
    "- - 0º OBTENEMOS la tabla **table = client.get_table(table)**\n",
    "  - 1º CAMBIAMOS la configuración **job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")**\n",
    "  - 2º   CARGAMOS DATOS (desde un DF en este caso) **job = client.load_table_from_dataframe(DF, table, job_config=job_config)** rage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7f7d1-df65-4d65-b442-87b2f330848e",
   "metadata": {},
   "source": [
    "#### CREAR TABLA E INSERTAR DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d2dee-64fe-4172-be13-92c0b8daada4",
   "metadata": {},
   "source": [
    "#### configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e167f2e-7199-4158-ba12-e36d84b8402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables de configuración\n",
    "env = \"DEV\"\n",
    "config_file = open('../config/general_config_' + env + '.json')\n",
    "config_file = json.load(config_file)\n",
    "##BigQuery\n",
    "scopes = config_file['bigquery']['scopes']\n",
    "bq_credentials = config_file['bigquery']['bq_credentials']\n",
    "project_id = config_file['bigquery']['project_id']\n",
    "dataset_id = config_file['bigquery']['dataset_id']\n",
    "table_id = config_file['bigquery']['table_id']\n",
    "csv_path = config_file['bigquery']['csv_path']\n",
    "insert_method = config_file['bigquery']['insert_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "015f8d63-63ab-4037-a162-68eb082ea6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONEXION\n",
    "def bq_client(scopes, bq_credentials, project_id):\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(bq_credentials, scopes=scopes)\n",
    "        client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(\"Big query local conection wrong: \" + str(e) + \" try virtual conexion\")\n",
    "        #La VM tiene que tener los permisos adecuados --> dárselos como user o descargando el archivo JSON de la cuenta de servicio y colocándolo en la VM (export GOOGLE_APPLICATION_CREDENTIALS=\"/ruta/a/tu/credencial.json\")\n",
    "        client = bigquery.Client()\n",
    "        return client\n",
    "\n",
    "client = bq_client(scopes, bq_credentials, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9a0496-59f9-43b9-a3fb-8f8d6188486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCIAR LA TABLA\n",
    "def bq_table_ref(client,dataset_id,table_id):\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "        return table_ref\n",
    "    except Exception as e:\n",
    "        print(\"Error in Bigquery table reference: \" + str(e))\n",
    "\n",
    "table = bq_table_ref(client,dataset_id,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fbf1808-3043-4b9f-948d-ec44df709913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV a un DataFrame de Pandas\n",
    "def bq_csv_to_pandas(csv_path):\n",
    "    try:\n",
    "        bigquery_csv = pd.read_csv(csv_path, sep=\",\", index_col=False)\n",
    "        return bigquery_csv\n",
    "    except Exception as e:\n",
    "        print(\"Error importing the csv file\" + str(e))\n",
    "\n",
    "bq_csv_df = bq_csv_to_pandas(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3667c-bbfe-46f8-86a1-99985fdd1e06",
   "metadata": {},
   "source": [
    "##### a) SQL --> client.query(query, job_config=job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6f8fad-f61c-4fe5-b211-253484f537d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correr después de crear la query:\n",
    "def bq_query_execution(client, query, dataset_id, table_id, table):\n",
    "    try:\n",
    "        query_ejecutada = client.query(query)\n",
    "        resultado_query = query_ejecutada.result()\n",
    "\n",
    "        if \"CREATE TABLE\" in query:\n",
    "            return(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "        elif \"INSERT\" in query:\n",
    "            return(\"Values inserted into {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "        elif \"TRUNCATE TABLE\" in query:\n",
    "            return(\"TRUNCATE {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636fbd8-deb2-47e2-bdcb-7b742f775387",
   "metadata": {},
   "source": [
    "###### 1.A crear la tabla si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7fdbd0-b01a-4abf-97a9-8f50556aaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAR ESQUEMA DESDE EL CSV \n",
    "\n",
    "def bq_schema_for_query(bq_csv_df):\n",
    "    schema_for_query = {}\n",
    "\n",
    "    # Itera sobre las columnas y determina el tipo de datos\n",
    "    try:\n",
    "        for column in bq_csv_df.columns:\n",
    "            # Infiere el tipo de datos basándose en los tipos de pandas\n",
    "            if pd.api.types.is_string_dtype(bq_csv_df[column]):\n",
    "                schema_for_query[column] = 'STRING'\n",
    "            elif pd.api.types.is_numeric_dtype(bq_csv_df[column]):\n",
    "                if pd.api.types.is_float_dtype(bq_csv_df[column]):\n",
    "                    schema_for_query[column] = 'FLOAT64'\n",
    "                else:\n",
    "                    schema_for_query[column] = 'INT64'\n",
    "            else:\n",
    "                schema_for_query[column] = 'STRING'\n",
    "                \n",
    "        schema_for_query = re.sub(\"'|\\{|\\}|\\[|\\]|\\:\",\"\",str(schema_for_query))\n",
    "        #schema_for_query = re.sub(\"FLOAT\",\"FLOAT64\",str(schema_for_query))\n",
    "        #schema_for_query = re.sub(\"INTEGER\",\"INT64\",str(schema_for_query))\n",
    "\n",
    "        return schema_for_query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "bq_schema_for_query = bq_schema_for_query(bq_csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b9b15b-8cdd-4265-bdf4-75d516ceb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_schema_for_query2(bq_csv_df):\n",
    "    try:\n",
    "        return re.sub(\"'|\\{|\\}|\\[|\\]|\\:|dtype|\\(|\\)\",\"\",str(bq_csv_df.dtypes.replace(\"O\",\"string\").to_dict())) #en minúscula\n",
    "        #return re.sub(\"'|\\{|\\}|\\[|\\]|\\:|dtype|\\(|\\)\",\"\",str({key: str(values).upper() for key, values in bq_csv_df.dtypes.replace(\"O\",\"string\").items()})) #en mayúscula\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "\n",
    "bq_schema_for_query2 = bq_schema_for_query2(bq_csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a951500-4f80-4997-854b-58b79dd687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear la tabla si no existe --> https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language\n",
    "def bq_query_create_table(dataset_id,table_id,bq_schema_for_query):\n",
    "    query = \"\"\"CREATE TABLE IF NOT EXISTS `{}.{}`({});\"\"\".format(dataset_id,table_id,bq_schema_for_query)\n",
    "    return query\n",
    "    \n",
    "    #PARTITION BY fecha\n",
    "    #OPTIONS(\n",
    "    #    partition_expiration_days=100,\n",
    "    #    description=\"a table partitioned by fecha\");\n",
    "\n",
    "bq_query_create_table = bq_query_create_table(dataset_id,table_id,bq_schema_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90917f9f-a1d2-4600-9233-58abe3cd70bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Created table curso-bigquery-mide-403114.chicago_taxi_tips.cars_upload_test'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ejecución de la query\n",
    "bq_query_execution(client, bq_query_create_table, dataset_id, table_id, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87f4d2-fac0-487a-b7ff-acbd50f9b536",
   "metadata": {},
   "source": [
    "###### 2.A INSERTAR datos del csv en la tabla debajo de los ya existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f45fde-766c-47ef-a367-984a24ebf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_query_insert_values(bq_csv_df,dataset_id,table_id):\n",
    "    #crear columnas\n",
    "    columns_for_query = list(bq_csv_df.columns)\n",
    "    columns_for_query = re.sub(\"'|\\{|\\}|\\[|\\]|\\:\",\"\",str(columns_for_query))\n",
    "\n",
    "    #crear valores\n",
    "    values_for_query = str([tuple(i_value) for i_value in bq_csv_df.values.tolist()])\n",
    "    values_for_query = re.sub(\"\\[|\\]\",\"\",str(values_for_query))\n",
    "\n",
    "    #crear query\n",
    "    query = \"\"\"INSERT `{}.{}`({}) VALUES {}\"\"\".format(dataset_id,table_id,columns_for_query,values_for_query)\n",
    "    return query\n",
    "\n",
    "bq_query_insert_values = bq_query_insert_values(bq_csv_df,dataset_id,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811d3835-df59-494e-8bf9-57145ac15d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. DML queries are not allowed in the free tier. Set up a billing account to remove this restriction.\n",
      "\n",
      "Location: europe-southwest1\n",
      "Job ID: 77576816-80c9-4bac-b40e-70ac0680387a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bq_query_execution(client, bq_query_insert_values, dataset_id, table_id, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bedc2c-dbf3-48c1-a874-6d6ae0e71964",
   "metadata": {},
   "source": [
    "###### * Si queremos eliminar los datos antes de insertarlos tendríamos que hacer un TRUNCATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a82f868-928a-46ef-bc51-1ab45b2402b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_query_truncate_table(dataset_id,table_id):\n",
    "    query = \"\"\"TRUNCATE TABLE `{}.{}`\"\"\".format(dataset_id,table_id)\n",
    "    return query\n",
    "\n",
    "bq_query_truncate_table = bq_query_truncate_table(dataset_id,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06ecaac8-c9e3-4684-8d7e-639dd8e0a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. DML queries are not allowed in the free tier. Set up a billing account to remove this restriction.\n",
      "\n",
      "Location: europe-southwest1\n",
      "Job ID: 2a3626b1-5e82-421e-acc1-1dcfb7414ddd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bq_query_execution(client, bq_query_truncate_table, dataset_id, table_id, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226b815-99b2-419b-8444-2928e54de76d",
   "metadata": {},
   "source": [
    "##### b) python\n",
    "https://cloud.google.com/bigquery/docs/tables?hl=es-419\n",
    "https://github.com/googleapis/python-bigquery/blob/35627d145a41d57768f19d4392ef235928e00f72/samples/create_table_range_partitioned.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df89869-5faf-487f-a83b-f54117bf177a",
   "metadata": {},
   "source": [
    "###### 1.A crear la tabla a mano si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b163040-3294-4d73-a30d-76257adf5e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla cars_upload_test ya existe en el conjunto de datos chicago_taxi_tips.\n"
     ]
    }
   ],
   "source": [
    "def bq_python_create_table_static(client, table, dataset_id, table_id):\n",
    "    try:\n",
    "        client.get_table(table)\n",
    "        print(f\"La tabla {table_id} ya existe en el conjunto de datos {dataset_id}.\")\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e):\n",
    "            print(f\"La tabla {table_id} no existe en el conjunto de datos {dataset_id}.\")\n",
    "            # Crea la tabla si no existe\n",
    "            schema = [\n",
    "                bigquery.SchemaField(\"cars\", \"STRING\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"mpg\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "                bigquery.SchemaField(\"cyl\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"disp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"hp\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"drat\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"wt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"qsec\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"vs\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"am\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"gear\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"carb\", \"INTEGER\", mode=\"NULLABLE\")\n",
    "            ]\n",
    "            table = bigquery.Table(table, schema=schema)\n",
    "    \n",
    "            #para crear tabla particionada\n",
    "            #table.time_partitioning = bigquery.TimePartitioning(\n",
    "            #    type_=bigquery.TimePartitioningType.DAY,\n",
    "            #    field=\"date\",  # name of column to use for partitioning\n",
    "            #    expiration_ms=1000 * 60 * 60 * 24 * 90,\n",
    "            #)  # 90 days\n",
    "            \n",
    "            table = client.create_table(table)\n",
    "            return(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "        else:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "bq_python_create_table_static(client, table, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909c780-6e75-49f8-9a31-45d2f9e61bab",
   "metadata": {},
   "source": [
    "###### 1.B crear la tabla leyendo el schema del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f92349e-f889-4e3e-b224-9d91d4b9fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla cars_upload_test no existe en el conjunto de datos chicago_taxi_tips.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Created table curso-bigquery-mide-403114.chicago_taxi_tips.cars_upload_test'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bq_python_create_table_dynamic(client,bq_csv_df, table, dataset_id, table_id):\n",
    "\n",
    "    # Crea un diccionario para almacenar el esquema inferido\n",
    "    schema = {}\n",
    "    \n",
    "    # Itera sobre las columnas y determina el tipo de datos\n",
    "    try:\n",
    "        for column in bq_csv_df.columns:\n",
    "            # Infiera el tipo de datos basándose en los tipos de pandas\n",
    "            if pd.api.types.is_string_dtype(bq_csv_df[column]):\n",
    "                schema[column] = 'STRING'\n",
    "            elif pd.api.types.is_numeric_dtype(bq_csv_df[column]):\n",
    "                if pd.api.types.is_float_dtype(bq_csv_df[column]):\n",
    "                    schema[column] = 'FLOAT'\n",
    "                else:\n",
    "                    schema[column] = 'INTEGER'\n",
    "            else:\n",
    "                schema[column] = 'STRING'  # Puedes ajustar según sea necesario\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    # iterar este esquema para crear el formato que BQ necesita en un listado (ejemplo mas arriba en la creacion a mano del listado)\n",
    "    try:\n",
    "        schema2=[bigquery.SchemaField(keyx, valuex, mode=\"NULLABLE\") for keyx,valuex in schema.items()]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        client.get_table(table)\n",
    "        print(f\"La tabla {table_id} ya existe en el conjunto de datos {dataset_id}.\")\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e):\n",
    "            print(f\"La tabla {table_id} no existe en el conjunto de datos {dataset_id}.\")\n",
    "            \n",
    "            # Crea la tabla si no existe\n",
    "            table = bigquery.Table(table, schema=schema2)\n",
    "    \n",
    "            #para crear tabla particionada\n",
    "            #table.time_partitioning = bigquery.TimePartitioning(\n",
    "            #    type_=bigquery.TimePartitioningType.DAY,\n",
    "            #    field=\"date\",  # name of column to use for partitioning\n",
    "            #    expiration_ms=1000 * 60 * 60 * 24 * 90,\n",
    "            #)  # 90 days\n",
    "            \n",
    "            table = client.create_table(table)\n",
    "            return(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "\n",
    "bq_python_create_table_dynamic(client,bq_csv_df, table, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d87eb6-f382-4a05-a093-9751674f2f40",
   "metadata": {},
   "source": [
    "###### 2.A INSERTAR datos del csv en la tabla debajo de los ya existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c9bbcb-b645-46eb-8ec7-3041e0085d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla cars_upload_test existe en el conjunto de datos chicago_taxi_tips.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Datos del CSV cargados en la tabla cars_upload_test de BigQuery.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bq_python_insert_values(client, bq_csv_df, table, dataset_id, table_id):\n",
    "    try:\n",
    "        table = client.get_table(table)\n",
    "        print(f\"La tabla {table_id} existe en el conjunto de datos {dataset_id}.\")\n",
    "    \n",
    "        job = client.load_table_from_dataframe(bq_csv_df, table)\n",
    "        job.result()  # Espera a que se complete la carga\n",
    "    \n",
    "        return(f\"Datos del CSV cargados en la tabla {table_id} de BigQuery.\")\n",
    "    except:\n",
    "        print(f\"La tabla {table_id} NO existe en el conjunto de datos {dataset_id}, hay que crearla\")\n",
    "\n",
    "bq_python_insert_values(client, bq_csv_df, table, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc4264-b0c5-495f-9aec-7725d824ddb1",
   "metadata": {},
   "source": [
    "###### 2.B REEMPLAZAR datos ya existentes en la tabla por los del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b4ab0d-efb5-4ace-b715-95bc8e0de0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla cars_upload_test existe en el conjunto de datos chicago_taxi_tips.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Datos del CSV reemplazaron los existentes en la tabla cars_upload_test de BigQuery.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bq_python_truncate_table(client, bq_csv_df, table, dataset_id, table_id):\n",
    "    try:\n",
    "        table = client.get_table(table)\n",
    "        print(f\"La tabla {table_id} existe en el conjunto de datos {dataset_id}.\")\n",
    "\n",
    "        #truncate\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "        #insertar datos\n",
    "        job = client.load_table_from_dataframe(bq_csv_df, table, job_config=job_config)\n",
    "        job.result()  # Espera a que se complete la carga\n",
    "    \n",
    "        return(f\"Datos del CSV reemplazaron los existentes en la tabla {table_id} de BigQuery.\")\n",
    "    \n",
    "    except:\n",
    "        print(f\"La tabla {table_id} NO existe en el conjunto de datos {dataset_id}, hay que crearla\")\n",
    "\n",
    "bq_python_truncate_table(client, bq_csv_df, table, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333458e-ed88-4cba-b9b0-888ee412b424",
   "metadata": {},
   "source": [
    "##### c) pandas_gbq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3c16b-2b0c-496b-ad6f-4662ac197b69",
   "metadata": {},
   "source": [
    "**DataFrame.to_gbq(*destination_table, project_id=None, chunksize=None, reauth=False, if_exists='fail', auth_local_webserver=True, table_schema=None, location=None, progress_bar=True, credentials=None*)**\n",
    "\n",
    "[DODUMENTACIÓN OFICIAL](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_gbq.html)\n",
    "\n",
    "**if_exists**: str, default ‘fail’\n",
    "\n",
    "--> Behavior when the destination table exists. Value can be one of:\n",
    "\n",
    "- **'fail'**\n",
    "If table exists raise pandas_gbq.gbq.TableCreationError.\n",
    "\n",
    "- **'replace'**\n",
    "If table exists, drop it, recreate it, and insert data.\n",
    "\n",
    "- **'append'**\n",
    "If table exists, insert data. Create if does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddaf21dc-d2e4-4160-898a-eaf51678b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_python_to_gbp_create_and_replace_values(client, bq_csv_df, project_id, dataset_id, table_id, insert_method):     \n",
    "    try:\n",
    "        to_gbq(bq_csv_df, f'{project_id}.{dataset_id}.{table_id}', project_id=project_id, if_exists=insert_method)\n",
    "        return f\"La tabla {table_id} ha sido creada y los datos del CSV han sido cargados en BigQuery.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c87268d0-3c43-48f7-a9dc-9c5fc46ac52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La tabla cars_upload_test ha sido creada y los datos del CSV han sido cargados en BigQuery.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_python_to_gbp_create_and_replace_values(client, bq_csv_df, project_id, dataset_id, table_id, insert_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048f73b-b94b-4a8c-88c4-d9ab31312e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
