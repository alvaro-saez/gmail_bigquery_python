{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf1b9c2-6280-440b-b70d-f919fa033c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import LoadJobConfig\n",
    "from pandas_gbq import to_gbq\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google_auth_oauthlib import flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9a165-72a0-4774-a8e8-7b02d0e232ed",
   "metadata": {},
   "source": [
    "## Funciones:\n",
    "\n",
    "- **credentials = service_account.Credentials.from_service_account_file(bq_credentials, scopes=scopes)**\n",
    "- **client = bigquery.Client(credentials=credentials, project=credentials.project_id)**   --> para establecer la conexion\n",
    "---\n",
    "- **client.dataset(dataset_id)**  --> para referenciar el data set\n",
    "- **dataset = client.create_dataset('us_states_dataset')** --> sino existe el dataset lo podemos crear\n",
    "- **client.table(table_id)**   --> para referenciar la tabla\n",
    "- **table = client.create_table(table)** --> sino existe la tabla la podemos crear\n",
    "- o referenciarla de golpe:\n",
    "- **table = client.dataset(dataset_id).table(table_id)** --> para referenciar el dataset y la tabla en conjunto\n",
    "- en el caso de que si que exista:\n",
    "- **table = client.get_table(table)** --> obtenemos la tabla de BigQuery\n",
    "---\n",
    "- **job_config = bigquery.job.QueryJobConfig(use_query_cache=False)**   --> para modificar configuraciones\n",
    "- **job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")**  --> para modificar configuraciones\n",
    "- **job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON** --> para seguir añadiendo modificaciones configuraciones\n",
    "---\n",
    "- **client.query(query, job_config=job_config)** --> para ejecutar una query\n",
    "---\n",
    "- **client.load_table_from_uri(gcs_uri, table, job_config=job_config)** --> para cargar una tabla a BQ desde un bucket de GCS\n",
    "---\n",
    "- CREACION DE TABLA\n",
    "- - **table = client.dataset(dataset_id).table(table_id)** --> referenciamos la tabla que deberia existir en BigQuery\n",
    "  - 0º COMPROBAMOS  a ver si existe **client.get_table(table)** --> obtenemos la tabla de BigQuery. SINO EXISTE LA TENDRIAMOS QUE CREAR:\n",
    "  - 1º DEFINIMOS el schema --> **schema = [bigquery.SchemaField(\"cars\", \"STRING\", mode=\"NULLABLE\"), bigquery.SchemaField(\"mpg\", \"FLOAT\", mode=\"REQUIRED\"),  ...]**\n",
    "  - 2º DEFINIMOS la tabla --> **table = bigquery.Table(table, schema=schema)**\n",
    "  - 3º CREAMOS la tabla --> **table = client.create_table(table)**\n",
    "  - también se podría saltar el paso 1 y 2 y configurar el schema en el jobconfig\n",
    "  - - **job_config = job_config.schema = [bigquery.SchemaField('name', 'STRING'), bigquery.SchemaField('post_abbr','STRING')]**\n",
    "    - **table = client.create_table(table, job_config=job_config)**\n",
    "---\n",
    "- INSERCION DE VALORES (debajo de los ya existentes) (partimos de que ya existe)\n",
    "- - 0º OBTENEMOS la tabla **table = client.get_table(table)**\n",
    "  - 1º CARGAMOS DATOS (desde un DF en este caso) **job = client.load_table_from_dataframe(DF, table)**\n",
    "--- \n",
    "- SOBREESCRIBIR VALORES\n",
    "- - 0º OBTENEMOS la tabla **table = client.get_table(table)**\n",
    "  - 1º CAMBIAMOS la configuración **job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")**\n",
    "  - 2º   CARGAMOS DATOS (desde un DF en este caso) **job = client.load_table_from_dataframe(DF, table, job_config=job_config)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03040ab6-2fc8-4f25-bca0-86fec308600c",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae646cd-024d-45b3-b7ec-65627d146d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables de configuración\n",
    "config_env = open('../config/general_config_environments.json')\n",
    "config_env = json.load(config_env)\n",
    "env = config_env['general']['config_env']\n",
    "\n",
    "config_file = open('../config/general_config_' + env + '.json')\n",
    "config_file = json.load(config_file)\n",
    "##BigQuery\n",
    "scopes = config_file['bigquery']['scopes']\n",
    "bq_credentials = config_file['bigquery']['bq_credentials']\n",
    "project_id = config_file['bigquery']['project_id']\n",
    "dataset_id = config_file['bigquery']['dataset_id']\n",
    "table_id = config_file['bigquery']['table_id']\n",
    "csv_path = config_file['bigquery']['csv_path']\n",
    "insert_method = config_file['bigquery']['insert_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510959b0-6497-432f-b4ab-b4580a929186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONEXION\n",
    "def bq_client(scopes, bq_credentials, project_id):\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(bq_credentials, scopes=scopes)\n",
    "        client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(\"Big query local conection wrong: \" + str(e) + \" try virtual conexion\")\n",
    "        #La VM tiene que tener los permisos adecuados --> dárselos como user o descargando el archivo JSON de la cuenta de servicio y colocándolo en la VM (export GOOGLE_APPLICATION_CREDENTIALS=\"/ruta/a/tu/credencial.json\")\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        return client\n",
    "\n",
    "client = bq_client(scopes, bq_credentials, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13231b12-9cce-4fce-9f49-1e0aadc7520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCIAR LA TABLA\n",
    "def bq_table_ref(client,dataset_id,table_id):\n",
    "    try:\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "        return table_ref\n",
    "    except Exception as e:\n",
    "        print(\"Error in Bigquery table reference: \" + str(e))\n",
    "\n",
    "table = bq_table_ref(client,dataset_id,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfd31db-d08a-475c-b833-e12903dbfb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV a un DataFrame de Pandas\n",
    "def bq_csv_to_pandas(csv_path):\n",
    "    try:\n",
    "        bigquery_csv = pd.read_csv(csv_path, sep=\",\", index_col=False)\n",
    "        return bigquery_csv\n",
    "    except Exception as e:\n",
    "        print(\"Error importing the csv file\" + str(e))\n",
    "\n",
    "bq_csv_df = bq_csv_to_pandas(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69ac78-af5c-4273-9210-c435d7ca70e7",
   "metadata": {},
   "source": [
    "### CREAR Y SUBIR DATOS A BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abe4d02-bc3b-4871-9fe0-b7c4cd42b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_schema_for_query(bq_csv_df):\n",
    "    try:\n",
    "        schema = eval(re.sub(\"\\[|\\]|dtype|\\(|\\)\",\"\",str(bq_csv_df.dtypes.replace(\"O\",\"string\").to_dict())))\n",
    "        schema_bq = [bigquery.SchemaField(keyx, valuex, mode=\"NULLABLE\") for keyx,valuex in schema.items()]\n",
    "        return schema_bq\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "\n",
    "bq_schema_for_query = bq_schema_for_query(bq_csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba3d075-edaa-4d0a-9267-187b305542a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table cars_upload_test already exists in the dataset chicago_taxi_tips.\n"
     ]
    }
   ],
   "source": [
    "def bq_python_create_table_dynamic(client,bq_csv_df,bq_schema_for_query, table, dataset_id, table_id):\n",
    "    # Obtiene la tabla\n",
    "    try:\n",
    "        client.get_table(table)\n",
    "        print(f\"The table {table_id} already exists in the dataset {dataset_id}.\")\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e): \n",
    "            # Crea la tabla si no existe\n",
    "            table = bigquery.Table(table, schema=bq_schema_for_query)\n",
    "    \n",
    "            #para crear tabla particionada\n",
    "            #table.time_partitioning = bigquery.TimePartitioning(\n",
    "            #    type_=bigquery.TimePartitioningType.DAY,\n",
    "            #    field=\"date\",  # name of column to use for partitioning\n",
    "            #    expiration_ms=1000 * 60 * 60 * 24 * 90,\n",
    "            #)  # 90 days\n",
    "            \n",
    "            table = client.create_table(table)\n",
    "            return(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n",
    "\n",
    "bq_python_create_table_dynamic(client,bq_csv_df, bq_schema_for_query, table, dataset_id, table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02fa08a-0d57-49be-b5b1-42863a6d783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data replaced the existing table data: cars_upload_test\n"
     ]
    }
   ],
   "source": [
    "def bq_python_insert_method(client, bq_csv_df, table, dataset_id, table_id, insert_method):\n",
    "    if insert_method and insert_method == \"replace\":\n",
    "        def bq_python_truncate_table(client, bq_csv_df, table, dataset_id, table_id):\n",
    "            try:\n",
    "                table = client.get_table(table)\n",
    "                #print(f\"The table {table_id} already exists in the dataset {dataset_id}.\")\n",
    "        \n",
    "                #truncate\n",
    "                job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "        \n",
    "                #insertar datos\n",
    "                job = client.load_table_from_dataframe(bq_csv_df, table, job_config=job_config)\n",
    "                job.result()  # Espera a que se complete la carga\n",
    "            \n",
    "                print(f\"CSV data replaced the existing table data: {table_id}\")\n",
    "            \n",
    "            except:\n",
    "                print(f\"The table {table_id} does not exist in the dataset {dataset_id}, must be created\")\n",
    "                \n",
    "        bq_python_truncate_table(client, bq_csv_df, table, dataset_id, table_id)\n",
    "\n",
    "    elif insert_method and insert_method == \"append\":\n",
    "        def bq_python_insert_values(client, bq_csv_df, table, dataset_id, table_id):\n",
    "            try:\n",
    "                table = client.get_table(table)\n",
    "                #print(f\"The table {table_id} already exists in the dataset {dataset_id}.\")\n",
    "            \n",
    "                job = client.load_table_from_dataframe(bq_csv_df, table)\n",
    "                job.result()  # Espera a que se complete la carga\n",
    "            \n",
    "                print(f\"CSV data added to: {table_id}\")\n",
    "            except:\n",
    "                print(f\"The table {table_id} does not exist in the dataset {dataset_id}, must be created\")\n",
    "        \n",
    "        bq_python_insert_values(client, bq_csv_df, table, dataset_id, table_id)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"CSV data can not be uploaded, please review the config file and the insert_method variable or the ptyhon file\") \n",
    "\n",
    "bq_python_insert_method(client, bq_csv_df, table, dataset_id, table_id, insert_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c40ba-32d2-430e-b522-8444b6c4f872",
   "metadata": {},
   "source": [
    "## OPCIÓN LIBRERÍA PANDAS to_gbp\n",
    "\n",
    "def bq_python_to_gbp_create_and_replace_values(client, bq_csv_df, project_id, dataset_id, table_id, insert_method):     \n",
    "    try:\n",
    "        to_gbq(bq_csv_df, f'{project_id}.{dataset_id}.{table_id}', project_id=project_id, if_exists=insert_method)\n",
    "        return f\"La tabla {table_id} ha sido creada y los datos del CSV han sido cargados en BigQuery.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "bq_python_to_gbp_create_and_replace_values(client, bq_csv_df, project_id, dataset_id, table_id, insert_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7874ef-6046-4421-9a2c-6b78ea7ad3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
